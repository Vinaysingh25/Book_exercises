---

Book: 'An Introduction to Statistical Learning with Applications in R by Gareth James,Daniela Witten,Trevor Hastie,Robert Tibshirani'
Notebook title: "ISLR_Chapter3_linear_regression_algorithm_unfold"
Author: 'Vinayak Singh'
Version: v1.1
Output: '.Rmd file'
Data Sources: Advertising dataset
Motive: We will implement linear regression Algorithm
It will help people who think R functions works like a black box and want some more intution around the algorithm implementation.
We will cover all the assumptions this algorithm makes.

---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

## Import Advertising data (you can download from course learn page or with this sheet)
```{r}
file_name <- file.choose()
adv_data <- read.csv(file_name)
```

## In this example we will implement linear regression using adversiting datasets variables sales and TV
##Lets check the relationship to get some idea of relationship between the variables.

```{r}
attach(adv_data) ## Check why the regression line is not correct 
cat('The correlation between objects is', cor(sales,TV))
reg1 <- lm(sales~TV, data = adv_data)
plot(sales,TV)

```

## Since we the regression line goes from the mean of both the values so for our reference we draw the mean line
```{r}
with(adv_data, plot(sales,TV))
abline(h=mean(TV))
abline(v=mean(sales))

```

## In this example our independent variable (X variable) is TV and dependent variable is sales(Y variable)
to calculate RSS (Residual Sum of square) we need beta0 and beta1
lets calculate beta1 first


```{r}
X = TV
Y = sales
x_mean = mean(X)
y_mean = mean(Y)
Xi_minus_x_mean = X-x_mean ## this returns the vector of all X values - mean of x
Yi_minus_y_mean = Y-y_mean ## this returns the vector of all Y values - mean of y
Xi_minus_x_mean_sqr = (X-x_mean)^2
beta1 = sum(Xi_minus_x_mean * Yi_minus_y_mean)/sum(Xi_minus_x_mean_sqr)

```

## now lets calculate the beta0 a.k.a coefficients (Refer to equation 3.4 in the book)

β0 = ¯y − ˆ β1¯x,
```{r}
beta0 = (y_mean - (beta1*x_mean))
```

## Now lets calculate residuals (e)
```{r}
Yhat = beta0 + beta1*X
model_residuals = Y-Yhat
```

##Lets draw a contor plot as shown in book for our values (figure 3.2)
##Bug: the Beta0 axis values are not correct but i will fix it 

```{r}
set.seed(22)
par(mfrow=c(1,1),mar=c(5,5,2,2))
g=50
x_cont <- X-mean(X)
y_cont <- Y
b <- sum((x_cont-mean(x_cont))*(y_cont-mean(y_cont)))/sum((x_cont-mean(x_cont))^2)
a <- mean(y_cont)-b*mean(x_cont)
RSS.min <- sum((y_cont-as.vector(cbind(1,x_cont)%*%c(a,b)))^2)/100000
a.grid <- seq(a-2,a+2,length=g)
b.grid <- seq(b-.02,b+.02,length=g)
grid <- as.matrix(expand.grid(a.grid,b.grid))

RSS <- rep(0,g^2)
for (i in 1:(g^2)){
y_hat=as.vector(cbind(1,x_cont)%*%grid[i,])
RSS[i]=sum((y_cont-y_hat)^2)/1000
}
RSS <- matrix(RSS,g,g)
m <- which.min(RSS)

contour(a.grid-b*mean(y_cont),b.grid,RSS,xlab=expression(beta[0]),ylab=expression(beta[1]),levels=c(2.11,2.15,2.2,2.3,2.5,3),axes=T,frame.plot=T,col=4,drawlabels=T,cex.lab=1.5,labcex=1.3)

points(a-b*mean(y_cont),b,col=2,pch=19,cex=1.5)
```

##Now lets check the residual error destribution 
```{r}
res_density <- density(model_residuals)
plot(res_density, main = 'Model residual error distribution')
##### These 2 lines are to test the distribution of the residual error please skip if not familier with the concept.
#shapiro.test(model_residuals)
#qqnorm(model_residuals); qqline(model_residuals, col = 2)

```

## μ = ¯y (Refer to formula 3.7 in the book)
```{r}
std_error_y_mean <- (sd(Y))/sqrt(length(Y))

```

## Standard error for beta0 & beta1 (Refer to formula 3.8 in the book)
```{r}

se_beta0 = sqrt(var(model_residuals)*(1/length(X) + ((x_mean)^2/sum((Xi_minus_x_mean)^2))))
se_beta1 = sqrt(var(model_residuals)/sum((Xi_minus_x_mean)^2))

```

##confidence interval for beta0 and beta1 (Refer to equation 3.10 in the book)
```{r}
print('The 95% confidence interval for beta0 is: ')
(beta0 - (2* se_beta0))
(beta0 + (2* se_beta0))
print('The 95% confidence interval for beta1 is: ')
(beta1-(2*se_beta1))
(beta1+(2*se_beta1))
```

## Now we perform hypothesis test (Refer to equation 3.14 in the book)
Null hypothesis 
H0 : There is no relationship between X and Y
H0 : β1 = 0
Alternative hypothesis
Ha : There is some relationship between X and Y
Ha : β1 != 0

```{r}
t_value_4_beta0 = (beta0-0)/(se_beta0)
t_value_4_beta1 = (beta1-0)/(se_beta1)
```

## Calculating RSS (Residual Standard Error)(Refer to equation 3.15 in the book)
```{r} 
#first lets calculate RSS that we can use for RSE equation
rss <- sum((Y-Yhat)^2)
n = length(Y)
rse <- sqrt((1/(n-2))*rss)
```

## Now lets calculate Rsquare Statistics (Refer to equation 3.17 in the book)
```{r}
## first we need to calculate TSS to plug into Rsquare formula 
tss <- sum((Y-y_mean)^2)
r_square <- (1 - (rss/tss))
```


